{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#visiblement la fonction \"sent_tokenize\" n'est pas définie par défaut sur jupyter-lab, on va l'importer avec la librairie NLTK,\n",
    "#et tous les modules nécessaires à faire run le code [documentation sur internet]\n",
    "%pip install nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lire(path): #ici, on définit la fonction \"lire(path)\" qui va chercher les fichiers .txt dans notre corpus \"Proust\"\n",
    "    with open(path, encoding= 'utf-8') as input_stream: #la fonction va lire le fichier avec utf-8 comme encodage pour pouvoir bien afficher\n",
    "#les caractères spéciaux et les accents que l'on peut trouver en français. Ce que cette fonction va lire sera stocké dans la variable \"input_stream\"\n",
    "        return [tokenizer.tokenize(sentence) for sentence in sent_tokenize(input_stream.read())] #ici la fonction \"tokenize\" va découper le contenu \n",
    "#qui se trouve dans la variable \"input_stream\" en liste de phrases, qui elles mêmes seront ensuite découpées en tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/farah/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/share/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m corpus = [] \u001b[38;5;66;03m#ici on crée un liste vide \"corpus\" où on ajoutera le contenu des fichiers .txt avec la fonction \"extend\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m corpus.extend(\u001b[43mlire\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/farah/Documents/Markov_projet/Proust/2998-0.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      3\u001b[39m corpus.extend(lire(\u001b[33m\"\u001b[39m\u001b[33m/Users/farah/Documents/Markov_projet/Proust/2999-0.txt\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      4\u001b[39m corpus.extend(lire(\u001b[33m\"\u001b[39m\u001b[33m/Users/farah/Documents/Markov_projet/Proust/3000-0.txt\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mlire\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlire\u001b[39m(path): \u001b[38;5;66;03m#ici, on définit la fonction \"lire(path)\" qui va chercher les fichiers .txt dans notre corpus \"Proust\"\u001b[39;00m\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, encoding= \u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m input_stream: \u001b[38;5;66;03m#la fonction va lire le fichier avec utf-8 comme encodage pour pouvoir bien afficher\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#les caractères spéciaux et les accents que l'on peut trouver en français. Ce que cette fonction va lire sera stocké dans la variable \"input_stream\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [tokenizer.tokenize(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/farah/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/share/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.9/libexec/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "corpus = [] #ici on crée un liste vide \"corpus\" où on ajoutera le contenu des fichiers .txt avec la fonction \"extend\"\n",
    "corpus.extend(lire(\"/Users/farah/Documents/Markov_projet/Proust/2998-0.txt\"))\n",
    "corpus.extend(lire(\"/Users/farah/Documents/Markov_projet/Proust/2999-0.txt\"))\n",
    "corpus.extend(lire(\"/Users/farah/Documents/Markov_projet/Proust/3000-0.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1 : chaîne de Markov de premier ordre (unigrammes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Compter les transitions\n",
    "\n",
    "Écrivez une fonction `compter_transitions_unigrammes` qui, étant donné un corpus (séquences de phrases, une phrase étant une séquence de mot), va compter les différentes transitions d'un mot à l'autre dans le corpus.\n",
    "\n",
    "Le résultat prendra la forme d'un dictionnaire à deux niveaux :\n",
    "- Le premier niveau sera le mot précédent\n",
    "- le second niveau sera le comptage du mot courant étant donné le mot précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compter_transitions_unigrammes(corpus):\n",
    "    transitions = {}\n",
    "\n",
    "    for phrase in corpus:\n",
    "        prev = \"\"\n",
    "        for token in phrase:\n",
    "            if prev not in transitions:\n",
    "                transitions[prev] = {}\n",
    "            if token not in transitions[prev]: #ici, on vérifie si c'est la première fois que le mot actuel (le token) apparaît après le mot précédent. \n",
    "# Le dictionnaire vide {} va stocker les mots qui ont déjà été suivi par \"prev\" \n",
    "                transitions[prev][token] = 0  #ici, si l'association des deux tokens est nouvelle, le compteur est initialisé à 0\n",
    "            transitions[prev][token] += 1     #ici, on augmente le compteur de 1 à chaque transition prev -> token\n",
    "            prev = token                      #ici, le mot actuel devient le mot précédent, c'est la méthode markovienne\n",
    "\n",
    "    return transitions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Transformer des comptes en probabilité\n",
    "\n",
    "Écrivez une fonction `probabilifier` qui, étant donné des comptes de transitions, transforme ces comptes en probabilité.\n",
    "\n",
    "La somme des probabilités des transitions partant d'un même mot doit sommer à 1 (ou presque 1, les nombres étant petits, des imprécisions peuvent arriver).\n",
    "\n",
    "Par exemple, la somme des transitions partant du mot `Si` doit sommer à 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilifier(comptes_transitions):\n",
    "    for token in comptes_transitions:\n",
    "        total = sum(comptes_transitions[token].values()) #ici, on calcule le nombre total des mots ayant suivi le token\n",
    "        for nxt in comptes_transitions[token]:           #ici, la boucle for parcourt chaque transition possible après ce même token\n",
    "            comptes_transitions[token][nxt] /= total     #ici, on divise le nombre d'apparitions du couple \"token + ntx\" par le nombre total d'apparitions de token.\n",
    "# c'est un calcul de la probabilité d'apparition du couple\n",
    "    return comptes_transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Créer une chaîne de Markov d'ordre 1\n",
    "\n",
    "En utilisant les fonctions précédentes, écrivez une fonction `chaine_markov_unigramme` qui, étant donné un corpus, renvoie une chaîne de Markov d'ordre 1 (probabilités de transitions d'un mot à un autre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chaine_markov_unigramme(corpus):\n",
    "    transitions = compter_transitions_unigrammes(corpus)\n",
    "\n",
    "    return probabilifier(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chain = chaine_markov_unigramme(corpus) \n",
    "\n",
    "#ici, on fait appel à la fonction définie au-dessus qui va pouvoir combiner les deux étapes du code précedent : \n",
    "#c'est à dire compter les occurrences puis les probabilités\n",
    "#ici, on utilise \"unigramme\" puisqu'il ne garde qu'un seul mot pour prédire le mot suivant, donc le mot suivant dépend du mot précédent\n",
    "#on crée donc une chaîne de Markov d'ordre 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Générer des phrases avec votre chaîne de Markov\n",
    "\n",
    "Écrivez une fonction `generer_unigramme` qui, étant donné un token de départ, va générer une phrase.\n",
    "\n",
    "On considère que nous sommes à la fin d'une phrase dès que l'on atteint une ponctuation forte ('.', '?', '!').\n",
    "\n",
    "Pour générer le prochain mot, on prendra systématiquement le mot le plus probable étant donné le mot précédent.\n",
    "\n",
    "Afin de vérifier le bon fonctionnement de la fonction, vous générerez d'abord une phrase commençant par `Si`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_unigramme(markov_chain, start_token):\n",
    "    maximum = NB_MOTS_MAXI\n",
    "    token = start_token\n",
    "    print(token, end=\" \")\n",
    "    prev = token\n",
    "    while token not in ponctuation and maximum > 0: #ici, la boucle s'arrête si il y a ponctuation finale ou si elle atteint la limite des mots définis\n",
    "        nxt = sorted([x for x in markov_chain[prev].items()], key=lambda x: -x[1]) #ici, on va trier les suites par probabilité décroissante (-x)\n",
    "        token = nxt[0][0] #ici la chaîne choisira le mot le plus probable\n",
    "        prev = token #ici, on met à jour la mémoire pour pouvoir prédire le mot suivant\n",
    "        maximum -= 1 #ici, on réduit à chaque fois le compteur de 1 pour éviter les boucles infinies\n",
    "        print(token, end=\" \") #ici, il affichera le mot généré suivi d'un espace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NB_MOTS_MAXI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerer_unigramme\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkov_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#*** modifiez la cellule ci-dessus pour n'afficher que 50 mots (là il y en 100 en output)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[32m30\u001b[39m*\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m generer_unigramme(markov_chain, \u001b[33m\"\u001b[39m\u001b[33mEt\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m#*** expliquez pourquoi cela marche\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mgenerer_unigramme\u001b[39m\u001b[34m(markov_chain, start_token)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerer_unigramme\u001b[39m(markov_chain, start_token):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     maximum = \u001b[43mNB_MOTS_MAXI\u001b[49m\n\u001b[32m      3\u001b[39m     token = start_token\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(token, end=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'NB_MOTS_MAXI' is not defined"
     ]
    }
   ],
   "source": [
    "NB_MOTS_MAXI = 50 #ici, on définit une variable globale pour avoir un output maximal de 50 mots\n",
    "\n",
    "generer_unigramme(markov_chain, \"Si\")\n",
    "print (30*'*')\n",
    "generer_unigramme(markov_chain, \"Et\") #ceci marche car \"Et\" est présent dans le corpus d'entraînement, le modèle que l'on a crée possède donc des données de transition pour ce token\n",
    "print (30*'*')\n",
    "generer_unigramme(markov_chain, \"Luc\") #ceci ne marche pas car ce token est absent dans le corpus, et le modèle n'a donc pas de données pour le traiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Améliorer la génération\n",
    "\n",
    "Comme vous avez pu le constater, la génération est assez... décevante. Pour améliorer la génération automatique, nous allons ajouter un peu d'aléatoire.\n",
    "\n",
    "Modifiez la fonction `generer_unigramme` pour y ajouter un argument `n_best` qui dira de prendre non pas le mot le plus probable, mais un mot au hasard parmi les `n_best` plus probables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_unigramme(markov_chain, start_token, n_best=1):\n",
    "    maximum = NB_MOTS_MAXI\n",
    "    token = start_token\n",
    "    print(token, end=\" \")\n",
    "    prev = token\n",
    "    while token not in ponctuation and maximum > 0:\n",
    "        nxt = sorted([x for x in markov_chain[prev].items()], key=lambda x: -x[1])\n",
    "        token = random.choice(nxt[:n_best])[0] #ici, cette fonction va picoher au hasard (random) un token parmis les n_best étant les plus probables\n",
    "        prev = token #ici, le mot généré devient le nouveau contexte pour les autres mots, ce qui permettra de prédire d'autres par la suite\n",
    "        maximum -= 1 #ici, c'est aussi pour éviter la boucle infinie\n",
    "        print(token, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et comme une fois que je n' était pas , mais qui n' était pas , et de la vie , mais qui n' est pas , et qui n' était plus de l' autre , mais qui n' était pas , et de l' autre , mais je ne me semblait pas , et de l' air d' une autre part , et de l' air d' une autre part , mais qui ne me semblait que je ne me dit que je ne me semblait pas , mais qui ne me semblait pas de la vie , et qui "
     ]
    }
   ],
   "source": [
    "generer_unigramme(markov_chain, \"Et\", 2) #ici, on fait appel à la fonction que l'on a définit au-dessus, avec \"Et\" comme token de départ\n",
    "#le \"2\" veut dire que le modèle prendra en compte le contexte, donc tire au sort entre les deux mots les plus probables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si l' on a pu croire pendant une grande dame à Balbec pour les jeunes qui n' étaient pas que vous a eu de la plus en avait pas la même pas la plus d' être plus de l' avait eu pour le temps . "
     ]
    }
   ],
   "source": [
    "generer_unigramme(markov_chain, \"Si\", 10)\n",
    "\n",
    "#ici, il prendra pour le token de départ \"Si\" les 10 mots les plus probables\n",
    "#donc on accroît petit à petit, la diversité lexicale du texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2 : chaîne de Markov d'ordre 2 (bigrammes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons fini par avoir une génération acceptable, mais elle demeure encore un peu incohérente. Afin d'améliorer la cohérence de la génération des mots, nous allons utiliser un contexte plus grand afin de générer les mots.\n",
    "\n",
    "Reprenez les fonctions précédentes (à part `probabilifier` qui restera la même) afin d'utiliser non pas un mot, mais deux mots en contexte.\n",
    "\n",
    "Si deux mots sont utilisés comme contexte, ils doivent être séparés par une espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compter_transitions_bigrammes(corpus):\n",
    "    transitions = {}\n",
    "\n",
    "    for phrase in corpus:\n",
    "        prevs = [\"\", \"\"] #ici, on initialise une mémoire de deux mots vides au début de chaque phrase pour stocker le contexte\n",
    "        for token in phrase:\n",
    "            prev = \" \".join(prevs) #ici, la fonction va fusionner les mots stockés en une seule chaîne de caractères pour créer un clé unique (l'état actuel)\n",
    "            if prev not in transitions: #ici, on vérifie si la combinaison de ces deux tokens a déjà été rencontré dans le corpus par le modèle\n",
    "                transitions[prev] = {} #si c'est pas le cas, on crée le dictionnaire pour répertorier les mots qui vont suivre\n",
    "            if token not in transitions[prev]: #ici, on vérifie si le token actuel a déjà été observé\n",
    "                transitions[prev][token] = 0 #ici, on initialise le compteur\n",
    "            transitions[prev][token] += 1 #ici, il ajoute de 1 le compteur pour chaque transition nouvelle\n",
    "            prevs[0] = prevs[1] #ici, on met à jour la mémoire du modèle, le mot 2 devient le 1 \n",
    "            prevs[1] = token #ici le mot actuel devient le 2 pour la prochaine étape\n",
    "\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chaine_markov_bigramme(corpus):\n",
    "    transitions = compter_transitions_bigrammes(corpus)\n",
    "\n",
    "    return probabilifier(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chain = chaine_markov_bigramme(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bi(markov_chain, start_token):\n",
    "    maximum = NB_MOTS_MAXI\n",
    "    token = start_token\n",
    "    print(token, end=\" \")\n",
    "    prevs = [\"\", \"\"]\n",
    "    prevs[1] = token\n",
    "    while token not in ponctuation and maximum > 0:\n",
    "        prev = \" \".join(prevs)\n",
    "        nxt = sorted([x for x in markov_chain[prev].items()], key=lambda x: -x[1])\n",
    "        token = nxt[0][0]\n",
    "        prevs[0] = prevs[1]\n",
    "        prevs[1] = token\n",
    "        maximum -= 1\n",
    "        print(token, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si j' avais été sur la plage , nous ne sommes pas accoutumés , le jour où je ne pouvais pas venir , au contraire , l' idée de moi , mais qui ne sont pas les mêmes que font travailler pour elles , et qui , à la fois , et qui , à la fois , et qui , à la fois , et qui , à la fois , et qui , à la fois , et qui , à la fois , et qui , à la fois , et qui , à la fois , et "
     ]
    }
   ],
   "source": [
    "generate_bi(markov_chain, \"Si\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bi(markov_chain, start_token, n_best=1):\n",
    "    maximum = NB_MOTS_MAXI\n",
    "    token = start_token\n",
    "    print(token, end=\" \")\n",
    "    prevs = [\"\", \"\"]\n",
    "    prevs[1] = token\n",
    "    while token not in ponctuation and maximum > 0:\n",
    "        prev = \" \".join(prevs)\n",
    "        nxt = sorted([x for x in markov_chain[prev].items()], key=lambda x: -x[1])\n",
    "        token = random.choice(nxt[:n_best])[0]\n",
    "        prevs[0] = prevs[1]\n",
    "        prevs[1] = token\n",
    "        maximum -= 1\n",
    "        print(token, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si j' avais été voir des tempêtes que sur un ton d' indifférence qu' on ne le connaît pas , et que j' étais venu chercher dans ses livres , les jours où je n' avais jamais songé qu' il y a une de ces jeunes gens--on en verra qui étaient venus la voir . "
     ]
    }
   ],
   "source": [
    "generate_bi(markov_chain, \"Si\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si je l' eusse reprise , cette heure d' air et en Amérique , ou comme à une soirée musicale qui perdrait de son visage était plus particulièrement câline avec Swann et que la photographie préférée de Swann qu' il était bien entendu ) avec quelle malice il a prononcé à l' entendre parler des tableaux achetés on ne s' y embrouille dans ces cortèges , un homme qui a cherché de toutes , ni d' autre aux femmes , tâchent de les aggraver , plus d' importance pour moi à Gilberte en faisant semblant d' ignorer que vous , Odette "
     ]
    }
   ],
   "source": [
    "generate_bi(markov_chain, \"Si\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 3 : chaîne de Markov d'ordre arbitraire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous y sommes presque ! Nous commençons à avoir une génération de qualité acceptable.\n",
    "\n",
    "Afin de rendre votre programme plus générique et plus puissant, réécrivez les fonctions précédentes afin qu'elles puissent créer des chaînes de Markov de n'importe quel ordre.\n",
    "\n",
    "L'ordre de la chaîne de Markov devra être rajouté aux fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compter_transitions(corpus, ordre):\n",
    "    transitions = {}\n",
    "\n",
    "    for phrase in corpus:\n",
    "        prevs = [\"\" for _ in range(ordre)] #ici, on crée une liste de mots vide dont la taille correspondra à l'ordre choisi \n",
    "        for token in phrase:\n",
    "            prev = \" \".join(prevs) #ici, la fonction va fusionner le nombre de tokens du contexte pour définir l'état présent\n",
    "            if prev not in transitions:\n",
    "                transitions[prev] = {}\n",
    "            if token not in transitions[prev]:\n",
    "                transitions[prev][token] = 0\n",
    "            transitions[prev][token] += 1\n",
    "            prevs = prevs[1:] #ici, ça supprime le mot le plus ancien dans le modèle\n",
    "            prevs.append(token) #ici, on ajoute le mot actuel à la fin\n",
    "\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chaine_markov(corpus, ordre):\n",
    "    transitions = compter_transitions(corpus, ordre)\n",
    "\n",
    "    return probabilifier(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_chain = chaine_markov(corpus, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer(markov_chain, ordre, start_token): #ici, on définit une fonction adaptable à n'importe quel n-gramme\n",
    "    maximum = NB_MOTS_MAXI\n",
    "    token = start_token\n",
    "    print(token, end=\" \")\n",
    "    prevs = [\"\" for _ in range(ordre)]\n",
    "    prevs[-1] = token\n",
    "    while token not in ponctuation and maximum > 0:\n",
    "        prev = \" \".join(prevs)\n",
    "        nxt = sorted([x for x in markov_chain[prev].items()], key=lambda x: -x[1])\n",
    "        token = nxt[0][0]\n",
    "        prevs = prevs[1:]\n",
    "        prevs.append(token)\n",
    "        maximum -= 1\n",
    "        print(token, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La personne du reste qui était le plus complètement dupe de l' illusion qui m' abusait ainsi que mes parents , c' était une chose à laquelle j' avais constamment songé , une chose toute en pensées , c' était une chose à laquelle j' avais constamment songé , une chose toute en pensées , c' était une chose à laquelle j' avais constamment songé , une chose toute en pensées , c' était une chose à laquelle j' avais constamment songé , une chose toute en pensées , c' était une chose à laquelle j' avais constamment songé , une "
     ]
    }
   ],
   "source": [
    "generer(markov_chain, 3, \"La\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer(markov_chain, ordre, start_token, n_best=1):\n",
    "    maximum = NB_MOTS_MAXI\n",
    "    token = start_token\n",
    "    print(token, end=\" \")\n",
    "    prevs = [\"\" for _ in range(ordre)]\n",
    "    prevs[-1] = token\n",
    "    while token not in ponctuation and maximum > 0:\n",
    "        prev = \" \".join(prevs)\n",
    "        nxt = sorted([x for x in markov_chain[prev].items()], key=lambda x: -x[1])\n",
    "        token = random.choice(nxt[:n_best])[0]\n",
    "        prevs = prevs[1:]\n",
    "        prevs.append(token)\n",
    "        maximum -= 1\n",
    "        print(token, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si j' étais monsieur votre père , je ne sais pas si c' est beaucoup ! "
     ]
    }
   ],
   "source": [
    "generer(markov_chain, 3, \"Si\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si les efforts de sincérité et d' émancipation de Saint-Loup ne pouvaient être trouvés que très nobles , à juger par le résultat extérieur , il était fort à craindre qu' elle se contentât de répondre de loin à ses vieux clients un clignement d' œil significatif . "
     ]
    }
   ],
   "source": [
    "generer(markov_chain, 3, \"Si\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depuis les débuts d' Elstir , nous avons bien trouvé qu' elle marquait très mal mais nous ne savions pas qu' elle ait ses cheveux comme ça , ça donne mauvais genre . "
     ]
    }
   ],
   "source": [
    "generer(markov_chain, 3, \"Depuis\", 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 4 : générer plus qu'une seule phrase (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les exercices précédents, on générait une seule phrase. Afin de compléter votre générateur, il faut à présent lui ajouter la possibilité de générer plusieurs phrases d'affilée !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Se souvenir des mots de la phrase précédente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une limitation à la génération que nous avons faite jusqu'ici est que les phrases sont vues indépendamment les unes des autres. Cela empêche la génération d'un texte plus complet qui serait constitué de plusieurs phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si l' on n' eût distingué deux yeux plus brillants que les autres . "
     ]
    }
   ],
   "source": [
    "generer(markov_chain, 3, \"Si\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si vous aimez les petites oies blanches . "
     ]
    }
   ],
   "source": [
    "generer(markov_chain, 3, \"Si\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
